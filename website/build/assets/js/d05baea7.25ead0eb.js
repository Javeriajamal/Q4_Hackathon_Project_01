"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[6232],{414:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"book/chapter-3/perception-and-sensing","title":"Perception and Sensing","description":"Learning Objectives","source":"@site/docs/book/chapter-3/perception-and-sensing.mdx","sourceDirName":"book/chapter-3","slug":"/book/chapter-3/perception-and-sensing","permalink":"/docs/book/chapter-3/perception-and-sensing","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/book/chapter-3/perception-and-sensing.mdx","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Perception and Sensing"},"sidebar":"tutorialSidebar","previous":{"title":"Control Systems and Actuation","permalink":"/docs/book/chapter-2/control-systems-and-actuation"},"next":{"title":"Decision Making and Learning","permalink":"/docs/book/chapter-3/decision-making-and-learning"}}');var r=i(4848),o=i(8453);const t={sidebar_position:1,title:"Perception and Sensing"},l="Perception and Sensing",a={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Main Content",id:"main-content",level:2},{value:"Vision Systems",id:"vision-systems",level:3},{value:"Camera Technologies",id:"camera-technologies",level:4},{value:"Computer Vision Algorithms",id:"computer-vision-algorithms",level:4},{value:"Tactile Sensing",id:"tactile-sensing",level:3},{value:"Tactile Sensor Technologies",id:"tactile-sensor-technologies",level:4},{value:"Auditory Perception",id:"auditory-perception",level:3},{value:"Microphone Arrays",id:"microphone-arrays",level:4},{value:"Speech Processing",id:"speech-processing",level:4},{value:"Sound Source Localization",id:"sound-source-localization",level:4},{value:"Proprioceptive Sensing",id:"proprioceptive-sensing",level:3},{value:"Joint Position Sensing",id:"joint-position-sensing",level:4},{value:"Inertial Measurement",id:"inertial-measurement",level:4},{value:"Force Sensing",id:"force-sensing",level:4},{value:"Multi-Modal Sensor Fusion",id:"multi-modal-sensor-fusion",level:3},{value:"Data Integration Approaches",id:"data-integration-approaches",level:4},{value:"Kalman Filtering",id:"kalman-filtering",level:4},{value:"Machine Learning Approaches",id:"machine-learning-approaches",level:4},{value:"Environmental Perception",id:"environmental-perception",level:3},{value:"3D Mapping",id:"3d-mapping",level:4},{value:"Object Recognition in 3D",id:"object-recognition-in-3d",level:4},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Resources",id:"further-resources",level:2}];function d(n){const e={h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"perception-and-sensing",children:"Perception and Sensing"})}),"\n",(0,r.jsx)(e.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Understand the sensory systems used in humanoid robots"}),"\n",(0,r.jsx)(e.li,{children:"Explore perception algorithms for environmental understanding"}),"\n",(0,r.jsx)(e.li,{children:"Analyze the integration of multiple sensory modalities"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"introduction",children:"Introduction"}),"\n",(0,r.jsx)(e.p,{children:"Perception and sensing form the foundation of a humanoid robot's ability to understand and interact with its environment. These systems enable robots to navigate spaces, recognize objects, understand human intentions, and adapt their behavior based on sensory input. The challenge lies in creating artificial perception systems that approach the robustness and efficiency of human sensory processing."}),"\n",(0,r.jsx)(e.h2,{id:"main-content",children:"Main Content"}),"\n",(0,r.jsx)(e.h3,{id:"vision-systems",children:"Vision Systems"}),"\n",(0,r.jsx)(e.h4,{id:"camera-technologies",children:"Camera Technologies"}),"\n",(0,r.jsx)(e.p,{children:"Different types of cameras used in humanoid robotics:"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"RGB Cameras"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Standard color imaging for object recognition"}),"\n",(0,r.jsx)(e.li,{children:"Real-time processing capabilities"}),"\n",(0,r.jsx)(e.li,{children:"Integration with deep learning models"}),"\n",(0,r.jsx)(e.li,{children:"Applications in facial recognition and scene understanding"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Depth Cameras"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"RGB-D sensors for 3D scene reconstruction"}),"\n",(0,r.jsx)(e.li,{children:"Time-of-flight cameras for distance measurement"}),"\n",(0,r.jsx)(e.li,{children:"Stereo vision systems for depth estimation"}),"\n",(0,r.jsx)(e.li,{children:"Applications in navigation and manipulation"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Event-Based Cameras"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Asynchronous pixel updates for dynamic scenes"}),"\n",(0,r.jsx)(e.li,{children:"High temporal resolution for fast motion"}),"\n",(0,r.jsx)(e.li,{children:"Low latency and low power consumption"}),"\n",(0,r.jsx)(e.li,{children:"Emerging technology for dynamic environments"}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"computer-vision-algorithms",children:"Computer Vision Algorithms"}),"\n",(0,r.jsx)(e.p,{children:"Processing visual information for robotic applications:"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Object Detection and Recognition"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Deep learning-based object detection"}),"\n",(0,r.jsx)(e.li,{children:"Real-time recognition systems"}),"\n",(0,r.jsx)(e.li,{children:"Category-level recognition"}),"\n",(0,r.jsx)(e.li,{children:"Instance-specific identification"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Scene Understanding"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Semantic segmentation of visual scenes"}),"\n",(0,r.jsx)(e.li,{children:"3D scene reconstruction from images"}),"\n",(0,r.jsx)(e.li,{children:"Spatial relationship understanding"}),"\n",(0,r.jsx)(e.li,{children:"Dynamic scene analysis"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Visual Tracking"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Multiple object tracking"}),"\n",(0,r.jsx)(e.li,{children:"Human pose estimation"}),"\n",(0,r.jsx)(e.li,{children:"Gaze tracking and attention"}),"\n",(0,r.jsx)(e.li,{children:"Visual servoing for manipulation"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"tactile-sensing",children:"Tactile Sensing"}),"\n",(0,r.jsx)(e.h4,{id:"tactile-sensor-technologies",children:"Tactile Sensor Technologies"}),"\n",(0,r.jsx)(e.p,{children:"Different approaches to artificial touch:"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Force/Torque Sensors"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Measurement of forces at contact points"}),"\n",(0,r.jsx)(e.li,{children:"Integration into robot hands and feet"}),"\n",(0,r.jsx)(e.li,{children:"Force control for safe interaction"}),"\n",(0,r.jsx)(e.li,{children:"Applications in grasping and manipulation"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Tactile Skin"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Distributed tactile sensing across surfaces"}),"\n",(0,r.jsx)(e.li,{children:"Pressure and texture detection"}),"\n",(0,r.jsx)(e.li,{children:"Temperature and slip sensing"}),"\n",(0,r.jsx)(e.li,{children:"Bio-inspired tactile sensor arrays"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"GelSight Technology"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"High-resolution tactile imaging"}),"\n",(0,r.jsx)(e.li,{children:"Fine texture and geometry detection"}),"\n",(0,r.jsx)(e.li,{children:"Applications in robotic manipulation"}),"\n",(0,r.jsx)(e.li,{children:"Real-time tactile feedback"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"auditory-perception",children:"Auditory Perception"}),"\n",(0,r.jsx)(e.h4,{id:"microphone-arrays",children:"Microphone Arrays"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Directional sound capture and localization"}),"\n",(0,r.jsx)(e.li,{children:"Noise reduction and speech enhancement"}),"\n",(0,r.jsx)(e.li,{children:"Multiple speaker tracking"}),"\n",(0,r.jsx)(e.li,{children:"Integration with spatial understanding"}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"speech-processing",children:"Speech Processing"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Automatic speech recognition (ASR)"}),"\n",(0,r.jsx)(e.li,{children:"Speaker identification and verification"}),"\n",(0,r.jsx)(e.li,{children:"Emotion recognition from voice"}),"\n",(0,r.jsx)(e.li,{children:"Natural language understanding"}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"sound-source-localization",children:"Sound Source Localization"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Acoustic scene analysis"}),"\n",(0,r.jsx)(e.li,{children:"Localization of sound sources"}),"\n",(0,r.jsx)(e.li,{children:"Integration with visual attention"}),"\n",(0,r.jsx)(e.li,{children:"Applications in human-robot interaction"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"proprioceptive-sensing",children:"Proprioceptive Sensing"}),"\n",(0,r.jsx)(e.h4,{id:"joint-position-sensing",children:"Joint Position Sensing"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"High-resolution encoders for joint angles"}),"\n",(0,r.jsx)(e.li,{children:"Absolute vs. incremental position sensing"}),"\n",(0,r.jsx)(e.li,{children:"Temperature compensation for accuracy"}),"\n",(0,r.jsx)(e.li,{children:"Integration with control systems"}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"inertial-measurement",children:"Inertial Measurement"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Accelerometers for linear motion detection"}),"\n",(0,r.jsx)(e.li,{children:"Gyroscopes for angular velocity measurement"}),"\n",(0,r.jsx)(e.li,{children:"Magnetometers for orientation reference"}),"\n",(0,r.jsx)(e.li,{children:"Sensor fusion for state estimation"}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"force-sensing",children:"Force Sensing"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Joint torque sensing for control"}),"\n",(0,r.jsx)(e.li,{children:"Ground reaction force measurement"}),"\n",(0,r.jsx)(e.li,{children:"Contact detection and classification"}),"\n",(0,r.jsx)(e.li,{children:"Impedance control applications"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"multi-modal-sensor-fusion",children:"Multi-Modal Sensor Fusion"}),"\n",(0,r.jsx)(e.h4,{id:"data-integration-approaches",children:"Data Integration Approaches"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Early fusion: Combining raw sensor data"}),"\n",(0,r.jsx)(e.li,{children:"Late fusion: Combining processed outputs"}),"\n",(0,r.jsx)(e.li,{children:"Deep fusion: Learning fusion representations"}),"\n",(0,r.jsx)(e.li,{children:"Probabilistic fusion methods"}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"kalman-filtering",children:"Kalman Filtering"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Linear and extended Kalman filters"}),"\n",(0,r.jsx)(e.li,{children:"Unscented Kalman filters"}),"\n",(0,r.jsx)(e.li,{children:"Particle filters for non-linear systems"}),"\n",(0,r.jsx)(e.li,{children:"Applications in state estimation"}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"machine-learning-approaches",children:"Machine Learning Approaches"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Deep learning for sensor fusion"}),"\n",(0,r.jsx)(e.li,{children:"Attention mechanisms for sensor selection"}),"\n",(0,r.jsx)(e.li,{children:"Graph neural networks for multi-sensor data"}),"\n",(0,r.jsx)(e.li,{children:"Learning-based fusion strategies"}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"environmental-perception",children:"Environmental Perception"}),"\n",(0,r.jsx)(e.h4,{id:"3d-mapping",children:"3D Mapping"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Simultaneous Localization and Mapping (SLAM)"}),"\n",(0,r.jsx)(e.li,{children:"Occupancy grid mapping"}),"\n",(0,r.jsx)(e.li,{children:"Feature-based mapping"}),"\n",(0,r.jsx)(e.li,{children:"Semantic mapping integration"}),"\n"]}),"\n",(0,r.jsx)(e.h4,{id:"object-recognition-in-3d",children:"Object Recognition in 3D"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"3D object detection and segmentation"}),"\n",(0,r.jsx)(e.li,{children:"Shape and pose estimation"}),"\n",(0,r.jsx)(e.li,{children:"Object affordance recognition"}),"\n",(0,r.jsx)(e.li,{children:"Scene understanding in 3D space"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,r.jsx)(e.p,{children:"The Pepper robot by SoftBank demonstrates sophisticated multimodal perception with its cameras, microphones, and touch sensors. The NAO robot includes cameras, microphones, and various sensors for environmental interaction. Advanced research platforms like the iCub robot feature extensive tactile sensing capabilities."}),"\n",(0,r.jsx)(e.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Multiple sensor modalities are essential for robust perception"}),"\n",(0,r.jsx)(e.li,{children:"Sensor fusion enables more accurate environmental understanding"}),"\n",(0,r.jsx)(e.li,{children:"Real-time processing requirements challenge perception systems"}),"\n",(0,r.jsx)(e.li,{children:"Bio-inspired approaches inform sensor design and algorithms"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"further-resources",children:"Further Resources"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'"Probabilistic Robotics" by Sebastian Thrun et al.'}),"\n",(0,r.jsx)(e.li,{children:"Computer Vision and Pattern Recognition (CVPR) conference proceedings"}),"\n",(0,r.jsx)(e.li,{children:"IEEE Transactions on Robotics special issues on perception"}),"\n"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>l});var s=i(6540);const r={},o=s.createContext(r);function t(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);