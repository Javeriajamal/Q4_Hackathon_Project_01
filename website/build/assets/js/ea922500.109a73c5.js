"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[2119],{1743:(n,i,e)=>{e.r(i),e.d(i,{assets:()=>t,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"book/chapter-3/decision-making-and-learning","title":"Decision Making and Learning","description":"Learning Objectives","source":"@site/docs/book/chapter-3/decision-making-and-learning.mdx","sourceDirName":"book/chapter-3","slug":"/book/chapter-3/decision-making-and-learning","permalink":"/docs/book/chapter-3/decision-making-and-learning","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/book/chapter-3/decision-making-and-learning.mdx","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Decision Making and Learning"},"sidebar":"tutorialSidebar","previous":{"title":"Perception and Sensing","permalink":"/docs/book/chapter-3/perception-and-sensing"},"next":{"title":"Industrial and Service Robotics","permalink":"/docs/book/chapter-4/industrial-and-service-robotics"}}');var a=e(4848),l=e(8453);const s={sidebar_position:2,title:"Decision Making and Learning"},o="Decision Making and Learning",t={},c=[{value:"Learning Objectives",id:"learning-objectives",level:2},{value:"Introduction",id:"introduction",level:2},{value:"Main Content",id:"main-content",level:2},{value:"Planning and Decision Making",id:"planning-and-decision-making",level:3},{value:"Motion Planning",id:"motion-planning",level:4},{value:"Task Planning",id:"task-planning",level:4},{value:"Behavior Selection",id:"behavior-selection",level:4},{value:"Machine Learning Approaches",id:"machine-learning-approaches",level:3},{value:"Supervised Learning",id:"supervised-learning",level:4},{value:"Reinforcement Learning",id:"reinforcement-learning",level:4},{value:"Imitation Learning",id:"imitation-learning",level:4},{value:"Real-Time Decision Making",id:"real-time-decision-making",level:3},{value:"Online Learning",id:"online-learning",level:4},{value:"Uncertainty Management",id:"uncertainty-management",level:4},{value:"Multi-Agent Coordination",id:"multi-agent-coordination",level:4},{value:"Learning from Interaction",id:"learning-from-interaction",level:3},{value:"Physical Interaction Learning",id:"physical-interaction-learning",level:4},{value:"Human-Robot Interaction Learning",id:"human-robot-interaction-learning",level:4},{value:"Skill Transfer and Generalization",id:"skill-transfer-and-generalization",level:4},{value:"Cognitive Architectures",id:"cognitive-architectures",level:3},{value:"Integrated Systems",id:"integrated-systems",level:4},{value:"Long-Term Learning",id:"long-term-learning",level:4},{value:"Safety and Ethical Considerations",id:"safety-and-ethical-considerations",level:3},{value:"Safe Learning",id:"safe-learning",level:4},{value:"Ethical Decision Making",id:"ethical-decision-making",level:4},{value:"Practical Examples",id:"practical-examples",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Resources",id:"further-resources",level:2}];function d(n){const i={h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,l.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(i.header,{children:(0,a.jsx)(i.h1,{id:"decision-making-and-learning",children:"Decision Making and Learning"})}),"\n",(0,a.jsx)(i.h2,{id:"learning-objectives",children:"Learning Objectives"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Understand AI algorithms for humanoid robot decision making"}),"\n",(0,a.jsx)(i.li,{children:"Explore machine learning approaches for robotic behavior"}),"\n",(0,a.jsx)(i.li,{children:"Analyze the integration of learning with real-time control"}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"introduction",children:"Introduction"}),"\n",(0,a.jsx)(i.p,{children:"Decision making and learning form the cognitive core of humanoid robots, enabling them to adapt to new situations, improve their performance over time, and make intelligent choices based on sensory input and goals. These systems bridge the gap between perception and action, allowing robots to operate autonomously in complex, dynamic environments."}),"\n",(0,a.jsx)(i.h2,{id:"main-content",children:"Main Content"}),"\n",(0,a.jsx)(i.h3,{id:"planning-and-decision-making",children:"Planning and Decision Making"}),"\n",(0,a.jsx)(i.h4,{id:"motion-planning",children:"Motion Planning"}),"\n",(0,a.jsx)(i.p,{children:"Algorithms for generating robot movement trajectories:"}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"Path Planning"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"A* and D* algorithms for optimal pathfinding"}),"\n",(0,a.jsx)(i.li,{children:"RRT (Rapidly-exploring Random Trees) for high-dimensional spaces"}),"\n",(0,a.jsx)(i.li,{children:"Visibility graphs for navigation in known environments"}),"\n",(0,a.jsx)(i.li,{children:"Dynamic path planning for moving obstacles"}),"\n"]}),"\n",(0,a.jsx)(i.p,{children:(0,a.jsx)(i.strong,{children:"Trajectory Optimization"})}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Model Predictive Control (MPC) for dynamic systems"}),"\n",(0,a.jsx)(i.li,{children:"Quadratic programming for optimal control"}),"\n",(0,a.jsx)(i.li,{children:"Real-time optimization for dynamic environments"}),"\n",(0,a.jsx)(i.li,{children:"Multi-objective optimization for competing goals"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"task-planning",children:"Task Planning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Hierarchical task networks (HTN) for complex tasks"}),"\n",(0,a.jsx)(i.li,{children:"STRIPS-based planning for symbolic actions"}),"\n",(0,a.jsx)(i.li,{children:"Partial order planning for flexible execution"}),"\n",(0,a.jsx)(i.li,{children:"Contingency planning for uncertain environments"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"behavior-selection",children:"Behavior Selection"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Finite state machines for behavior switching"}),"\n",(0,a.jsx)(i.li,{children:"Behavior trees for complex action selection"}),"\n",(0,a.jsx)(i.li,{children:"Utility-based decision making"}),"\n",(0,a.jsx)(i.li,{children:"Multi-criteria decision analysis"}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"machine-learning-approaches",children:"Machine Learning Approaches"}),"\n",(0,a.jsx)(i.h4,{id:"supervised-learning",children:"Supervised Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Classification for object recognition"}),"\n",(0,a.jsx)(i.li,{children:"Regression for sensor calibration"}),"\n",(0,a.jsx)(i.li,{children:"Deep learning for perception tasks"}),"\n",(0,a.jsx)(i.li,{children:"Transfer learning for new domains"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"reinforcement-learning",children:"Reinforcement Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Markov Decision Processes (MDPs) for sequential decisions"}),"\n",(0,a.jsx)(i.li,{children:"Q-learning for discrete action spaces"}),"\n",(0,a.jsx)(i.li,{children:"Deep Q-Networks (DQN) for complex state spaces"}),"\n",(0,a.jsx)(i.li,{children:"Policy gradient methods for continuous control"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"imitation-learning",children:"Imitation Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Learning from demonstration (LfD)"}),"\n",(0,a.jsx)(i.li,{children:"Behavioral cloning for skill acquisition"}),"\n",(0,a.jsx)(i.li,{children:"Inverse reinforcement learning for reward shaping"}),"\n",(0,a.jsx)(i.li,{children:"One-shot learning for rapid skill transfer"}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"real-time-decision-making",children:"Real-Time Decision Making"}),"\n",(0,a.jsx)(i.h4,{id:"online-learning",children:"Online Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Continuous adaptation to changing environments"}),"\n",(0,a.jsx)(i.li,{children:"Incremental learning algorithms"}),"\n",(0,a.jsx)(i.li,{children:"Catastrophic forgetting prevention"}),"\n",(0,a.jsx)(i.li,{children:"Balancing exploration and exploitation"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"uncertainty-management",children:"Uncertainty Management"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Probabilistic reasoning under uncertainty"}),"\n",(0,a.jsx)(i.li,{children:"Bayesian inference for belief updating"}),"\n",(0,a.jsx)(i.li,{children:"Monte Carlo methods for complex distributions"}),"\n",(0,a.jsx)(i.li,{children:"Decision making with confidence bounds"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"multi-agent-coordination",children:"Multi-Agent Coordination"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Distributed decision making"}),"\n",(0,a.jsx)(i.li,{children:"Communication protocols for coordination"}),"\n",(0,a.jsx)(i.li,{children:"Consensus algorithms for group decisions"}),"\n",(0,a.jsx)(i.li,{children:"Game theory for multi-agent interactions"}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"learning-from-interaction",children:"Learning from Interaction"}),"\n",(0,a.jsx)(i.h4,{id:"physical-interaction-learning",children:"Physical Interaction Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Learning through environmental interaction"}),"\n",(0,a.jsx)(i.li,{children:"Trial-and-error learning for manipulation"}),"\n",(0,a.jsx)(i.li,{children:"Physical simulation for safe learning"}),"\n",(0,a.jsx)(i.li,{children:"Transfer from simulation to reality (sim-to-real)"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"human-robot-interaction-learning",children:"Human-Robot Interaction Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Learning from human feedback"}),"\n",(0,a.jsx)(i.li,{children:"Social learning mechanisms"}),"\n",(0,a.jsx)(i.li,{children:"Collaborative learning scenarios"}),"\n",(0,a.jsx)(i.li,{children:"Adaptive interfaces based on user preferences"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"skill-transfer-and-generalization",children:"Skill Transfer and Generalization"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Transfer learning between tasks"}),"\n",(0,a.jsx)(i.li,{children:"Domain adaptation for new environments"}),"\n",(0,a.jsx)(i.li,{children:"Few-shot learning for rapid adaptation"}),"\n",(0,a.jsx)(i.li,{children:"Meta-learning for learning to learn"}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"cognitive-architectures",children:"Cognitive Architectures"}),"\n",(0,a.jsx)(i.h4,{id:"integrated-systems",children:"Integrated Systems"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Perception-action loops with learning"}),"\n",(0,a.jsx)(i.li,{children:"Memory systems for experience storage"}),"\n",(0,a.jsx)(i.li,{children:"Attention mechanisms for focus"}),"\n",(0,a.jsx)(i.li,{children:"Planning-execution-monitoring cycles"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"long-term-learning",children:"Long-Term Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Lifelong learning systems"}),"\n",(0,a.jsx)(i.li,{children:"Experience replay mechanisms"}),"\n",(0,a.jsx)(i.li,{children:"Knowledge transfer between tasks"}),"\n",(0,a.jsx)(i.li,{children:"Forgetting mechanisms for memory management"}),"\n"]}),"\n",(0,a.jsx)(i.h3,{id:"safety-and-ethical-considerations",children:"Safety and Ethical Considerations"}),"\n",(0,a.jsx)(i.h4,{id:"safe-learning",children:"Safe Learning"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Safe exploration algorithms"}),"\n",(0,a.jsx)(i.li,{children:"Constrained optimization for safety"}),"\n",(0,a.jsx)(i.li,{children:"Formal verification of learned policies"}),"\n",(0,a.jsx)(i.li,{children:"Human oversight in learning systems"}),"\n"]}),"\n",(0,a.jsx)(i.h4,{id:"ethical-decision-making",children:"Ethical Decision Making"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Incorporating ethical constraints"}),"\n",(0,a.jsx)(i.li,{children:"Value alignment with human preferences"}),"\n",(0,a.jsx)(i.li,{children:"Transparent decision making"}),"\n",(0,a.jsx)(i.li,{children:"Accountability in autonomous systems"}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"practical-examples",children:"Practical Examples"}),"\n",(0,a.jsx)(i.p,{children:"The DeepMind work on reinforcement learning for robotics demonstrates sophisticated learning in complex environments. The OpenAI Dactyl system shows dexterous manipulation learned through reinforcement learning. The ROS-Industrial initiative provides frameworks for integrating learning with robotic systems."}),"\n",(0,a.jsx)(i.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:"Decision making combines planning, control, and learning"}),"\n",(0,a.jsx)(i.li,{children:"Real-time constraints require efficient algorithms"}),"\n",(0,a.jsx)(i.li,{children:"Learning from interaction enables adaptation"}),"\n",(0,a.jsx)(i.li,{children:"Safety and ethics are critical considerations"}),"\n"]}),"\n",(0,a.jsx)(i.h2,{id:"further-resources",children:"Further Resources"}),"\n",(0,a.jsxs)(i.ul,{children:["\n",(0,a.jsx)(i.li,{children:'"Reinforcement Learning: An Introduction" by Sutton and Barto'}),"\n",(0,a.jsx)(i.li,{children:"IEEE Transactions on Robotics special issues on learning"}),"\n",(0,a.jsx)(i.li,{children:"Conference on Robot Learning (CoRL) proceedings"}),"\n"]})]})}function h(n={}){const{wrapper:i}={...(0,l.R)(),...n.components};return i?(0,a.jsx)(i,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453:(n,i,e)=>{e.d(i,{R:()=>s,x:()=>o});var r=e(6540);const a={},l=r.createContext(a);function s(n){const i=r.useContext(l);return r.useMemo(function(){return"function"==typeof n?n(i):{...i,...n}},[i,n])}function o(n){let i;return i=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),r.createElement(l.Provider,{value:i},n.children)}}}]);